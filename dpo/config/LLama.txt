Model:
meta-llama/Llama-3.3-70B-Instruct-Reference
Training file:
dpo_training.jsonl
Training type:
Full
Training method:
DPO
DPO beta:
0.1
RPO alpha:
0
Normalize log ratios:
false
Reference free:
false
SimPO gamma:
0
Epochs:
1
Checkpoints:
1
Evaluations:
1
Batch size:
16
Learning rate:
0.00001
Warmup ratio:
0
Max gradient norm:
1
Weight decay:
0
LR scheduler type:
cosine
Min LR ratio:
0
Scheduler cycles:
0.5