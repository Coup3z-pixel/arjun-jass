Model:
mistralai/Mixtral-8x7B-v0.1
Training file:
dpo_training.jsonl
Training type:
Full
Training method:
DPO
DPO beta:
0.1
RPO alpha:
0
Normalize log ratios:
false
Reference free:
false
SimPO gamma:
0
Epochs:
1
Checkpoints:
1
Evaluations:
1
Batch size:
16
Learning rate:
0.00001
Warmup ratio:
0
Max gradient norm:
1
Weight decay:
0
LR scheduler type:
cosine
Min LR ratio:
0
Scheduler cycles:
0.5